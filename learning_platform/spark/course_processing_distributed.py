"""
Module de traitement distribu√© des cours avec Apache Spark
Permet de traiter des milliers de cours en parall√®le avec classification ML distribu√©e
"""

import json
import logging
import os
from pathlib import Path

from pyspark.ml import Pipeline
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.feature import *
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DistributedCourseProcessor:
    """Processeur de cours distribu√© utilisant Apache Spark"""

    def __init__(self, spark_master_url="spark://spark-master:7077"):
        self.spark_master_url = spark_master_url
        self.spark = None
        self._init_spark_session()

    def _init_spark_session(self):
        """Initialise la session Spark"""
        try:
            self.spark = (
                SparkSession.builder.appName("InLearning-CourseProcessing")
                .master(self.spark_master_url)
                .config("spark.sql.adaptive.enabled", "true")
                .config("spark.sql.adaptive.coalescePartitions.enabled", "true")
                .config("spark.executor.memory", "2g")
                .config("spark.executor.cores", "2")
                .config("spark.driver.memory", "1g")
                .getOrCreate()
            )

            logger.info(
                f"‚úÖ Session Spark initialis√©e avec master: {self.spark_master_url}"
            )
            return True

        except Exception as e:
            logger.error(f"‚ùå Erreur initialisation Spark: {e}")
            return False

    def process_courses_from_json(self, json_path, output_path=None):
        """Traite un fichier JSON de cours de mani√®re distribu√©e"""
        try:
            logger.info(f"üöÄ D√©but traitement distribu√©: {json_path}")

            # Sch√©ma pour les cours
            course_schema = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("titre", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("contenus", MapType(StringType(), StringType()), True),
                    StructField("categories", ArrayType(StringType()), True),
                    StructField("niveau", StringType(), True),
                    StructField("duree", StringType(), True),
                ]
            )

            # Charger les donn√©es
            df = self.spark.read.option("multiline", "true").json(
                json_path, schema=course_schema
            )

            logger.info(f"üìä Nombre de cours charg√©s: {df.count()}")

            # Pr√©parer le texte pour la classification
            df_processed = df.withColumn(
                "full_text",
                concat_ws(
                    " ",
                    coalesce(col("titre"), lit("")),
                    coalesce(col("description"), lit("")),
                    coalesce(col("contenus.texte"), lit("")),
                ),
            ).filter(col("full_text") != "")

            # Classification distribu√©e
            classified_df = self._classify_courses_distributed(df_processed)

            # Pr√©diction de niveau distribu√©e
            final_df = self._predict_levels_distributed(classified_df)

            # Sauvegarder les r√©sultats
            if output_path:
                final_df.write.mode("overwrite").parquet(output_path)
                logger.info(f"üíæ R√©sultats sauvegard√©s: {output_path}")

            # Retourner les statistiques
            stats = self._compute_statistics(final_df)
            return stats

        except Exception as e:
            logger.error(f"‚ùå Erreur traitement distribu√©: {e}")
            return None

    def _classify_courses_distributed(self, df):
        """Classification distribu√©e des cours par cat√©gorie"""
        try:
            logger.info("ü§ñ Classification distribu√©e en cours...")

            # Tokenization
            tokenizer = Tokenizer(inputCol="full_text", outputCol="words")

            # Suppression des mots vides
            remover = StopWordsRemover(inputCol="words", outputCol="filtered_words")

            # TF-IDF
            hashingTF = HashingTF(
                inputCol="filtered_words", outputCol="rawFeatures", numFeatures=10000
            )
            idf = IDF(inputCol="rawFeatures", outputCol="features")

            # Pr√©parer les labels (simulation pour demo)
            categories = [
                "Python",
                "JavaScript",
                "Data Science",
                "Web Development",
                "Machine Learning",
                "Autres",
            ]
            df_with_labels = df.withColumn(
                "label_category",
                when(lower(col("full_text")).contains("python"), 0.0)
                .when(lower(col("full_text")).contains("javascript"), 1.0)
                .when(lower(col("full_text")).contains("data"), 2.0)
                .when(lower(col("full_text")).contains("web"), 3.0)
                .when(lower(col("full_text")).contains("machine"), 4.0)
                .otherwise(5.0),
            )

            # Pipeline de classification
            lr = LogisticRegression(featuresCol="features", labelCol="label_category")
            pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])

            # Entra√Ænement du mod√®le
            model = pipeline.fit(df_with_labels)

            # Pr√©dictions
            predictions = model.transform(df_with_labels)

            # Ajouter le nom de la cat√©gorie pr√©dite
            category_mapping = {i: cat for i, cat in enumerate(categories)}

            def map_category(category_id):
                return category_mapping.get(int(category_id), "Autres")

            map_category_udf = udf(map_category, StringType())

            result_df = predictions.withColumn(
                "predicted_category", map_category_udf(col("prediction"))
            ).withColumn(
                "category_confidence",
                round(col("probability").getItem(col("prediction").cast("int")), 3),
            )

            logger.info("‚úÖ Classification distribu√©e termin√©e")
            return result_df

        except Exception as e:
            logger.error(f"‚ùå Erreur classification: {e}")
            return df

    def _predict_levels_distributed(self, df):
        """Pr√©diction distribu√©e des niveaux de difficult√©"""
        try:
            logger.info("üìä Pr√©diction de niveaux distribu√©e...")

            # Simulation de pr√©diction de niveau bas√©e sur le contenu
            level_df = (
                df.withColumn(
                    "predicted_level",
                    when(
                        lower(col("full_text")).contains("d√©butant")
                        | lower(col("full_text")).contains("introduction")
                        | lower(col("full_text")).contains("basics"),
                        "D√©butant",
                    )
                    .when(
                        lower(col("full_text")).contains("avanc√©")
                        | lower(col("full_text")).contains("expert")
                        | lower(col("full_text")).contains("advanced"),
                        "Avanc√©",
                    )
                    .otherwise("Interm√©diaire"),
                )
                .withColumn(
                    "level_confidence",
                    rand() * 0.3 + 0.7,  # Simulation confidence entre 0.7 et 1.0
                )
                .withColumn("processing_timestamp", current_timestamp())
            )

            logger.info("‚úÖ Pr√©diction de niveaux termin√©e")
            return level_df

        except Exception as e:
            logger.error(f"‚ùå Erreur pr√©diction niveaux: {e}")
            return df

    def _compute_statistics(self, df):
        """Calcule les statistiques du traitement distribu√©"""
        try:
            total_courses = df.count()

            # Statistiques par cat√©gorie
            category_stats = df.groupBy("predicted_category").count().collect()

            # Statistiques par niveau
            level_stats = df.groupBy("predicted_level").count().collect()

            # Confidence moyenne
            avg_confidence = df.agg(
                avg("category_confidence").alias("avg_category_confidence"),
                avg("level_confidence").alias("avg_level_confidence"),
            ).collect()[0]

            stats = {
                "total_courses_processed": total_courses,
                "category_distribution": {
                    row["predicted_category"]: row["count"] for row in category_stats
                },
                "level_distribution": {
                    row["predicted_level"]: row["count"] for row in level_stats
                },
                "average_confidences": {
                    "category": float(avg_confidence["avg_category_confidence"]),
                    "level": float(avg_confidence["avg_level_confidence"]),
                },
            }

            logger.info(f"üìà Statistiques calcul√©es: {stats}")
            return stats

        except Exception as e:
            logger.error(f"‚ùå Erreur calcul statistiques: {e}")
            return {}

    def process_streaming_courses(self, input_path="/opt/bitnami/spark/ingest/drop"):
        """Traitement en streaming des nouveaux cours"""
        try:
            logger.info(f"üåä D√©marrage streaming depuis: {input_path}")

            # Schema pour les cours en streaming
            course_schema = StructType(
                [
                    StructField("id", StringType(), True),
                    StructField("titre", StringType(), True),
                    StructField("description", StringType(), True),
                    StructField("contenus", MapType(StringType(), StringType()), True),
                ]
            )

            # Stream depuis le r√©pertoire de d√©p√¥t
            stream_df = (
                self.spark.readStream.format("json")
                .schema(course_schema)
                .option("path", input_path)
                .load()
            )

            # Traitement en temps r√©el
            processed_stream = (
                stream_df.withColumn(
                    "full_text", concat_ws(" ", col("titre"), col("description"))
                )
                .withColumn("processing_time", current_timestamp())
                .withColumn("word_count", size(split(col("full_text"), " ")))
            )

            # Output vers console pour debugging
            query = (
                processed_stream.writeStream.outputMode("append")
                .format("console")
                .option("truncate", False)
                .trigger(processingTime="30 seconds")
                .start()
            )

            logger.info("‚úÖ Streaming d√©marr√©")
            return query

        except Exception as e:
            logger.error(f"‚ùå Erreur streaming: {e}")
            return None

    def close(self):
        """Ferme la session Spark"""
        if self.spark:
            self.spark.stop()
            logger.info("üîí Session Spark ferm√©e")


def main():
    """Fonction principale pour test du processeur distribu√©"""
    processor = DistributedCourseProcessor()

    try:
        # Test avec un fichier exemple
        input_path = "/opt/bitnami/spark/data/course_pipeline/courses.json"
        output_path = "/opt/bitnami/spark/data/processed_courses"

        if os.path.exists(input_path):
            stats = processor.process_courses_from_json(input_path, output_path)
            print(f"üìä R√©sultats du traitement distribu√©:")
            print(json.dumps(stats, indent=2))
        else:
            logger.warning(f"‚ö†Ô∏è Fichier non trouv√©: {input_path}")

    except Exception as e:
        logger.error(f"‚ùå Erreur main: {e}")
    finally:
        processor.close()


if __name__ == "__main__":
    main()
