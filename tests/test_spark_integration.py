"""
Tests d'intÃ©gration Apache Spark pour InLearning Platform
Valide le fonctionnement du cluster Spark et des traitements distribuÃ©s
"""

import unittest
import requests
import json
import time
import os
from pathlib import Path


class TestSparkIntegration(unittest.TestCase):
    """Tests d'intÃ©gration pour Apache Spark"""

    def setUp(self):
        self.spark_ui_url = "http://localhost:8090"  # Spark Master UI
        self.api_url = "http://localhost:5000"  # Flask API
        self.test_courses = [
            {
                "id": "spark_test_001",
                "titre": "Test Spark Processing",
                "description": "Cours de test pour validation Spark",
                "contenus": {
                    "texte": "Contenu de test pour traitement distribuÃ©",
                    "paragraphs": ["Introduction au test", "Validation Spark"],
                    "examples": ["exemple_code = 'test'"],
                },
                "categories": ["Test"],
                "niveau": "DÃ©butant",
                "duree": "1h",
            },
            {
                "id": "spark_test_002",
                "titre": "Machine Learning DistribuÃ©",
                "description": "Test ML avec Spark MLlib",
                "contenus": {
                    "texte": "Test des algorithmes ML distribuÃ©s",
                    "paragraphs": ["ML Pipeline", "Distributed Computing"],
                    "examples": ["from pyspark.ml import Pipeline"],
                },
                "categories": ["Machine Learning"],
                "niveau": "AvancÃ©",
                "duree": "2h",
            },
        ]

    def test_spark_cluster_available(self):
        """Test 1: VÃ©rifier que le cluster Spark est accessible"""
        print("\nğŸ” Test 1: DisponibilitÃ© du cluster Spark")

        try:
            response = requests.get(f"{self.spark_ui_url}/json/", timeout=10)
            self.assertEqual(response.status_code, 200)

            cluster_info = response.json()
            active_workers = cluster_info.get("aliveworkers", 0)

            print(f"âœ… Cluster Spark accessible")
            print(f"ğŸ“Š Workers actifs: {active_workers}")
            print(f"ğŸ’¾ MÃ©moire totale: {cluster_info.get('memory', 0)} MB")
            print(f"ğŸ–¥ï¸ Cores totaux: {cluster_info.get('cores', 0)}")

            self.assertGreaterEqual(
                active_workers, 1, "Au moins 1 worker doit Ãªtre actif"
            )

        except requests.exceptions.RequestException as e:
            self.fail(f"âŒ Cluster Spark non accessible: {e}")

    def test_api_spark_status(self):
        """Test 2: VÃ©rifier le statut Spark dans l'API Flask"""
        print("\nğŸ” Test 2: Statut Spark dans l'API")

        try:
            response = requests.get(f"{self.api_url}/status")
            self.assertEqual(response.status_code, 200)

            status_data = response.json()
            spark_cluster = status_data.get("spark_cluster", {})

            print(f"âœ… API accessible")
            print(f"ğŸš€ Spark activÃ©: {spark_cluster.get('enabled', False)}")
            print(f"ğŸ“Š Workers: {spark_cluster.get('active_workers', 0)}")
            print(f"ğŸ¯ Modes de traitement: {status_data.get('processing_modes', {})}")

            self.assertTrue(
                spark_cluster.get("enabled", False), "Spark devrait Ãªtre activÃ©"
            )

        except requests.exceptions.RequestException as e:
            self.fail(f"âŒ API non accessible: {e}")

    def test_distributed_course_processing(self):
        """Test 3: Traitement distribuÃ© de cours"""
        print("\nğŸ” Test 3: Traitement distribuÃ© de cours")

        try:
            # Soumettre les cours pour traitement distribuÃ©
            response = requests.post(
                f"{self.api_url}/process-courses-distributed",
                json=self.test_courses,
                headers={"Content-Type": "application/json"},
            )

            print(f"ğŸ“¤ Soumission: {response.status_code}")

            if response.status_code == 200:
                result = response.json()
                job_id = result.get("job_id")

                print(f"âœ… Job Spark soumis: {job_id}")
                print(f"ğŸ“Š Cours traitÃ©s: {result.get('course_count')}")
                print(f"â±ï¸ DurÃ©e estimÃ©e: {result.get('estimated_duration')}s")

                self.assertIsNotNone(job_id, "Job ID devrait Ãªtre retournÃ©")
                self.assertEqual(result.get("course_count"), len(self.test_courses))

                # Attendre et vÃ©rifier le statut du job
                self._wait_for_job_completion(job_id)

            else:
                # Fallback vers traitement local - acceptable
                print("âš ï¸ Fallback vers traitement local")
                self.assertIn(response.status_code, [200, 500])

        except requests.exceptions.RequestException as e:
            self.fail(f"âŒ Erreur traitement distribuÃ©: {e}")

    def test_spark_job_monitoring(self):
        """Test 4: Monitoring des jobs Spark"""
        print("\nğŸ” Test 4: Monitoring des jobs Spark")

        try:
            # Soumettre un job de test
            response = requests.post(
                f"{self.api_url}/process-courses-distributed",
                json=self.test_courses[:1],  # Un seul cours pour test rapide
                headers={"Content-Type": "application/json"},
            )

            if response.status_code == 200:
                result = response.json()
                job_id = result.get("job_id")

                if job_id:
                    print(f"ğŸ“Š Monitoring job: {job_id}")

                    # VÃ©rifier le statut du job
                    status_response = requests.get(
                        f"{self.api_url}/spark/job-status/{job_id}"
                    )

                    if status_response.status_code == 200:
                        status = status_response.json()
                        print(f"ğŸ“ˆ Statut job: {status.get('status')}")
                        print(
                            f"ğŸ†” Application ID: {status.get('application_id', 'N/A')}"
                        )

                        self.assertIn(
                            status.get("status"), ["running", "completed", "submitted"]
                        )
                    else:
                        print("âš ï¸ Statut job non accessible")

        except Exception as e:
            print(f"âš ï¸ Test monitoring: {e}")

    def test_spark_performance_metrics(self):
        """Test 5: MÃ©triques de performance Spark"""
        print("\nğŸ” Test 5: MÃ©triques de performance")

        try:
            response = requests.get(f"{self.api_url}/stats/processed-courses")
            self.assertEqual(response.status_code, 200)

            stats = response.json()
            spark_metrics = stats.get("spark_metrics", {})

            print(
                f"ğŸ“Š MÃ©triques Spark disponibles: {'Oui' if spark_metrics else 'Non'}"
            )

            if spark_metrics and "error" not in spark_metrics:
                print(
                    f"ğŸƒ Applications totales: {spark_metrics.get('total_applications', 0)}"
                )
                print(f"âœ… Jobs complÃ©tÃ©s: {spark_metrics.get('completed_jobs', 0)}")
                print(
                    f"â±ï¸ Temps total: {spark_metrics.get('total_processing_time', 0)}ms"
                )
                print(
                    f"ğŸ“ˆ Utilisation cluster: {spark_metrics.get('cluster_utilization', 'unknown')}"
                )
            else:
                print("âš ï¸ MÃ©triques Spark non disponibles")

        except requests.exceptions.RequestException as e:
            self.fail(f"âŒ Erreur mÃ©triques: {e}")

    def test_spark_ui_access(self):
        """Test 6: AccÃ¨s Ã  l'interface Spark"""
        print("\nğŸ” Test 6: Interface Spark UI")

        try:
            # Test page principale
            response = requests.get(f"{self.spark_ui_url}/", timeout=5)
            self.assertEqual(response.status_code, 200)

            # Test API applications
            api_response = requests.get(
                f"{self.spark_ui_url}/api/v1/applications", timeout=5
            )

            if api_response.status_code == 200:
                applications = api_response.json()
                print(f"âœ… Spark UI accessible")
                print(f"ğŸ“± Applications: {len(applications)}")

                for app in applications[:3]:  # Afficher max 3 apps
                    print(f"   - {app.get('name', 'Unknown')}: {app.get('id', 'N/A')}")
            else:
                print("âš ï¸ API Spark non accessible")

        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ Spark UI non accessible: {e}")

    def _wait_for_job_completion(self, job_id, max_wait=60):
        """Attend la completion d'un job Spark"""
        print(f"â³ Attente completion job {job_id}...")

        start_time = time.time()
        while time.time() - start_time < max_wait:
            try:
                response = requests.get(f"{self.api_url}/spark/job-status/{job_id}")
                if response.status_code == 200:
                    status = response.json()
                    job_status = status.get("status", "unknown")

                    if job_status in ["completed", "succeeded"]:
                        print(f"âœ… Job {job_id} terminÃ© avec succÃ¨s")
                        return True
                    elif job_status in ["failed", "error"]:
                        print(
                            f"âŒ Job {job_id} Ã©chouÃ©: {status.get('error', 'Unknown error')}"
                        )
                        return False

                    print(f"ğŸ”„ Job {job_id} en cours... ({job_status})")

                time.sleep(5)

            except Exception as e:
                print(f"âš ï¸ Erreur monitoring job: {e}")
                break

        print(f"â° Timeout waiting for job {job_id}")
        return False


def run_spark_integration_tests():
    """Lance tous les tests d'intÃ©gration Spark"""
    print("ğŸš€ === TESTS D'INTÃ‰GRATION APACHE SPARK ===\n")

    # Configuration du test
    loader = unittest.TestLoader()
    suite = loader.loadTestsFromTestCase(TestSparkIntegration)

    # ExÃ©cution des tests
    runner = unittest.TextTestRunner(verbosity=2)
    result = runner.run(suite)

    # RÃ©sumÃ©
    print("\nğŸ“Š === RÃ‰SUMÃ‰ DES TESTS SPARK ===")
    print(
        f"âœ… Tests rÃ©ussis: {result.testsRun - len(result.failures) - len(result.errors)}"
    )
    print(f"âŒ Tests Ã©chouÃ©s: {len(result.failures)}")
    print(f"ğŸš« Erreurs: {len(result.errors)}")

    if result.failures:
        print("\nâŒ Ã‰CHECS:")
        for test, traceback in result.failures:
            print(f"   - {test}: {traceback}")

    if result.errors:
        print("\nğŸš« ERREURS:")
        for test, traceback in result.errors:
            print(f"   - {test}: {traceback}")

    # Statut final
    if result.wasSuccessful():
        print("\nğŸ‰ TOUS LES TESTS SPARK SONT PASSÃ‰S!")
        return True
    else:
        print("\nâš ï¸ CERTAINS TESTS SPARK ONT Ã‰CHOUÃ‰")
        return False


if __name__ == "__main__":
    success = run_spark_integration_tests()
    exit(0 if success else 1)
